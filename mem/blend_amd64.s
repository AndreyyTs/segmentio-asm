// Code generated by command: go run blend_asm.go -pkg mem -out ../mem/blend_amd64.s -stubs ../mem/blend_amd64.go. DO NOT EDIT.

#include "textflag.h"

// func Blend(dst []byte, src []byte) int
// Requires: AVX, AVX2, CMOV
TEXT ·Blend(SB), NOSPLIT, $0-56
	MOVQ    dst_base+0(FP), AX
	MOVQ    src_base+24(FP), CX
	MOVQ    dst_len+8(FP), DX
	MOVQ    src_len+32(FP), BX
	CMPQ    BX, DX
	CMOVQGT BX, DX
	MOVQ    DX, ret+48(FP)

	// Tail copy with special cases for each possible item size.
tail:
	CMPQ DX, $0x00
	JE   done
	CMPQ DX, $0x01
	JE   copy1
	CMPQ DX, $0x02
	JE   copy2
	CMPQ DX, $0x03
	JE   copy3
	CMPQ DX, $0x04
	JE   copy4
	CMPQ DX, $0x05
	JE   copy5
	CMPQ DX, $0x06
	JE   copy6
	CMPQ DX, $0x07
	JE   copy7
	CMPQ DX, $0x08
	JE   copy8
	CMPQ DX, $0x20
	JB   generic
	BTL  $0x08, github·com∕segmentio∕asm∕cpu·X86+0(SB)
	JCS  avx2

	// Generic copy for short inputs or targets without AVX instructions.
generic:
	MOVQ (CX), BX
	ORQ  BX, (AX)
	ADDQ $0x08, CX
	ADDQ $0x08, AX
	SUBQ $0x08, DX
	CMPQ DX, $0x08
	JBE  tail
	JMP  generic

done:
	RET

copy1:
	MOVB (CX), CL
	ORB  CL, (AX)
	RET

copy2:
	MOVW (CX), CX
	ORW  CX, (AX)
	RET

copy3:
	MOVW (CX), DX
	ORW  DX, (AX)
	MOVB 2(CX), CL
	ORB  CL, 2(AX)
	RET

copy4:
	MOVL (CX), CX
	ORL  CX, (AX)
	RET

copy5:
	MOVL (CX), DX
	ORL  DX, (AX)
	MOVB 4(CX), CL
	ORB  CL, 4(AX)
	RET

copy6:
	MOVL (CX), DX
	ORL  DX, (AX)
	MOVW 4(CX), CX
	ORW  CX, 4(AX)
	RET

copy7:
	MOVL (CX), DX
	ORL  DX, (AX)
	MOVW 4(CX), DX
	ORW  DX, 4(AX)
	MOVB 6(CX), CL
	ORB  CL, 6(AX)
	RET

copy8:
	MOVQ (CX), CX
	ORQ  CX, (AX)
	RET

	// AVX optimized version for medium to large size inputs.
avx2:
	CMPQ    DX, $0x80
	JB      avx2_tail
	VMOVDQU (CX), Y0
	VMOVDQU 32(CX), Y1
	VMOVDQU 64(CX), Y2
	VMOVDQU 96(CX), Y3
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VPOR    64(AX), Y2, Y2
	VPOR    96(AX), Y3, Y3
	VMOVDQU Y0, (AX)
	VMOVDQU Y1, 32(AX)
	VMOVDQU Y2, 64(AX)
	VMOVDQU Y3, 96(AX)
	ADDQ    $0x80, AX
	ADDQ    $0x80, CX
	SUBQ    $0x80, DX
	JMP     avx2

avx2_tail:
	JZ      done
	CMPQ    DX, $0x20
	JBE     avx2_tail_1to32
	CMPQ    DX, $0x40
	JBE     avx2_tail_33to64
	CMPQ    DX, $0x60
	JBE     avx2_tail_65to96
	VMOVDQU (CX), Y0
	VMOVDQU 32(CX), Y1
	VMOVDQU 64(CX), Y2
	VMOVDQU -32(CX)(DX*1), Y3
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VPOR    64(AX), Y2, Y2
	VPOR    -32(AX)(DX*1), Y3, Y3
	VMOVDQU Y0, (AX)
	VMOVDQU Y1, 32(AX)
	VMOVDQU Y2, 64(AX)
	VMOVDQU Y3, -32(AX)(DX*1)
	RET

avx2_tail_65to96:
	VMOVDQU (CX), Y0
	VMOVDQU 32(CX), Y1
	VMOVDQU -32(CX)(DX*1), Y3
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VPOR    -32(AX)(DX*1), Y3, Y3
	VMOVDQU Y0, (AX)
	VMOVDQU Y1, 32(AX)
	VMOVDQU Y3, -32(AX)(DX*1)
	RET

avx2_tail_33to64:
	VMOVDQU (CX), Y0
	VMOVDQU -32(CX)(DX*1), Y3
	VPOR    (AX), Y0, Y0
	VPOR    -32(AX)(DX*1), Y3, Y3
	VMOVDQU Y0, (AX)
	VMOVDQU Y3, -32(AX)(DX*1)
	RET

avx2_tail_1to32:
	VMOVDQU -32(CX)(DX*1), Y3
	VPOR    -32(AX)(DX*1), Y3, Y3
	VMOVDQU Y3, -32(AX)(DX*1)
	RET
