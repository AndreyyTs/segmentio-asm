// Code generated by command: go run sort_asm.go -pkg qsort -out ../qsort/sort_amd64.s -stubs ../qsort/sort_amd64.go. DO NOT EDIT.

#include "textflag.h"

// func insertionsort128NoSwapAsm(data []byte)
// Requires: AVX, AVX2, SSE4.1
TEXT ·insertionsort128NoSwapAsm(SB), NOSPLIT, $0-24
	MOVQ         data_base+0(FP), AX
	MOVQ         data_len+8(FP), CX
	ADDQ         AX, CX
	TESTQ        AX, CX
	JE           done
	MOVQ         $0x8000000000000000, DX
	PINSRQ       $0x00, DX, X0
	VPBROADCASTQ X0, X0
	MOVQ         AX, DX

outer:
	ADDQ    $0x10, DX
	CMPQ    DX, CX
	JAE     done
	VMOVDQU (DX), X1
	MOVQ    DX, SI

inner:
	VMOVDQU   -16(SI), X2
	VPCMPEQQ  X1, X2, X3
	VPADDQ    X1, X0, X4
	VPADDQ    X2, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, DI
	VMOVMSKPD X4, R8
	NOTL      DI
	BSFL      DI, BX
	BTSL      BX, R8
	JAE       outer
	VMOVDQU   X2, (SI)
	VMOVDQU   X1, -16(SI)
	SUBQ      $0x10, SI
	CMPQ      SI, AX
	JA        inner
	JMP       outer

done:
	RET

// func distributeForward128(data *byte, scratch *byte, limit int, lo int, hi int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeForward128(SB), NOSPLIT, $0-48
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	SHLQ         $0x04, DX
	SHLQ         $0x04, BX
	SHLQ         $0x04, SI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	LEAQ         -16(CX)(DX*1), CX
	MOVQ         $0x8000000000000000, R8
	PINSRQ       $0x00, R8, X0
	VPBROADCASTQ X0, X0
	VMOVDQU      (AX), X1
	XORQ         R8, R8
	XORQ         R9, R9
	NEGQ         DX

loop:
	VMOVDQU   (BX), X2
	VPCMPEQQ  X2, X1, X3
	VPADDQ    X2, X0, X4
	VPADDQ    X1, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, R10
	VMOVMSKPD X4, R11
	NOTL      R10
	BSFL      R10, DI
	BTSL      DI, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	XORB      $0x01, R9
	MOVQ      BX, R10
	CMOVQNE   CX, R10
	VMOVDQU   X2, (R10)(R8*1)
	SHLQ      $0x04, R9
	SUBQ      R9, R8
	ADDQ      $0x10, BX
	CMPQ      BX, SI
	JA        done
	CMPQ      R8, DX
	JNE       loop

done:
	SUBQ AX, BX
	ADDQ R8, BX
	SHRQ $0x04, BX
	DECQ BX
	MOVQ BX, ret+40(FP)
	RET

// func distributeBackward128(data *byte, scratch *byte, limit int, lo int, hi int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeBackward128(SB), NOSPLIT, $0-48
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	SHLQ         $0x04, DX
	SHLQ         $0x04, BX
	SHLQ         $0x04, SI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	MOVQ         $0x8000000000000000, R8
	PINSRQ       $0x00, R8, X0
	VPBROADCASTQ X0, X0
	VMOVDQU      (AX), X1
	XORQ         R8, R8
	XORQ         R9, R9
	CMPQ         SI, BX
	JBE          done

loop:
	VMOVDQU   (SI), X2
	VPCMPEQQ  X2, X1, X3
	VPADDQ    X2, X0, X4
	VPADDQ    X1, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, R10
	VMOVMSKPD X4, R11
	NOTL      R10
	BSFL      R10, DI
	BTSL      DI, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	MOVQ      CX, R10
	CMOVQEQ   SI, R10
	VMOVDQU   X2, (R10)(R8*1)
	SHLQ      $0x04, R9
	ADDQ      R9, R8
	SUBQ      $0x10, SI
	CMPQ      SI, BX
	JBE       done
	CMPQ      R8, DX
	JNE       loop

done:
	SUBQ AX, SI
	ADDQ R8, SI
	SHRQ $0x04, SI
	MOVQ SI, ret+40(FP)
	RET

// func insertionsort256NoSwapAsm(data []byte)
// Requires: AVX, AVX2, SSE4.1
TEXT ·insertionsort256NoSwapAsm(SB), NOSPLIT, $0-24
	MOVQ         data_base+0(FP), AX
	MOVQ         data_len+8(FP), CX
	ADDQ         AX, CX
	TESTQ        AX, CX
	JE           done
	MOVQ         $0x8000000000000000, DX
	PINSRQ       $0x00, DX, X0
	VPBROADCASTQ X0, Y0
	MOVQ         AX, DX

outer:
	ADDQ    $0x20, DX
	CMPQ    DX, CX
	JAE     done
	VMOVDQU (DX), Y1
	MOVQ    DX, SI

inner:
	VMOVDQU   -32(SI), Y2
	VPCMPEQQ  Y1, Y2, Y3
	VPADDQ    Y1, Y0, Y4
	VPADDQ    Y2, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, DI
	VMOVMSKPD Y4, R8
	NOTL      DI
	BSFL      DI, BX
	BTSL      BX, R8
	JAE       outer
	VMOVDQU   Y2, (SI)
	VMOVDQU   Y1, -32(SI)
	SUBQ      $0x20, SI
	CMPQ      SI, AX
	JA        inner
	JMP       outer

done:
	VZEROUPPER
	RET

// func distributeForward256(data *byte, scratch *byte, limit int, lo int, hi int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeForward256(SB), NOSPLIT, $0-48
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	SHLQ         $0x05, DX
	SHLQ         $0x05, BX
	SHLQ         $0x05, SI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	LEAQ         -32(CX)(DX*1), CX
	MOVQ         $0x8000000000000000, R8
	PINSRQ       $0x00, R8, X0
	VPBROADCASTQ X0, Y0
	VMOVDQU      (AX), Y1
	XORQ         R8, R8
	XORQ         R9, R9
	NEGQ         DX

loop:
	VMOVDQU   (BX), Y2
	VPCMPEQQ  Y2, Y1, Y3
	VPADDQ    Y2, Y0, Y4
	VPADDQ    Y1, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, R10
	VMOVMSKPD Y4, R11
	NOTL      R10
	BSFL      R10, DI
	BTSL      DI, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	XORB      $0x01, R9
	MOVQ      BX, R10
	CMOVQNE   CX, R10
	VMOVDQU   Y2, (R10)(R8*1)
	SHLQ      $0x05, R9
	SUBQ      R9, R8
	ADDQ      $0x20, BX
	CMPQ      BX, SI
	JA        done
	CMPQ      R8, DX
	JNE       loop

done:
	SUBQ AX, BX
	ADDQ R8, BX
	SHRQ $0x05, BX
	DECQ BX
	MOVQ BX, ret+40(FP)
	VZEROUPPER
	RET

// func distributeBackward256(data *byte, scratch *byte, limit int, lo int, hi int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeBackward256(SB), NOSPLIT, $0-48
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	SHLQ         $0x05, DX
	SHLQ         $0x05, BX
	SHLQ         $0x05, SI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	MOVQ         $0x8000000000000000, R8
	PINSRQ       $0x00, R8, X0
	VPBROADCASTQ X0, Y0
	VMOVDQU      (AX), Y1
	XORQ         R8, R8
	XORQ         R9, R9
	CMPQ         SI, BX
	JBE          done

loop:
	VMOVDQU   (SI), Y2
	VPCMPEQQ  Y2, Y1, Y3
	VPADDQ    Y2, Y0, Y4
	VPADDQ    Y1, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, R10
	VMOVMSKPD Y4, R11
	NOTL      R10
	BSFL      R10, DI
	BTSL      DI, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	MOVQ      CX, R10
	CMOVQEQ   SI, R10
	VMOVDQU   Y2, (R10)(R8*1)
	SHLQ      $0x05, R9
	ADDQ      R9, R8
	SUBQ      $0x20, SI
	CMPQ      SI, BX
	JBE       done
	CMPQ      R8, DX
	JNE       loop

done:
	SUBQ AX, SI
	ADDQ R8, SI
	SHRQ $0x05, SI
	MOVQ SI, ret+40(FP)
	VZEROUPPER
	RET
