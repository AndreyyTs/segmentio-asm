// Code generated by command: go run hybrid_asm.go -pkg qsort -out ../qsort/hybrid_amd64.s -stubs ../qsort/hybrid_amd64.go. DO NOT EDIT.

#include "textflag.h"

// func medianOfThree16(data *byte, a int, b int, c int)
// Requires: AVX, AVX2, SSE4.1
TEXT ·medianOfThree16(SB), NOSPLIT, $0-32
	MOVQ         data+0(FP), AX
	MOVQ         a+8(FP), CX
	MOVQ         b+16(FP), DX
	MOVQ         c+24(FP), BX
	SHLQ         $0x04, CX
	SHLQ         $0x04, DX
	SHLQ         $0x04, BX
	ADDQ         AX, CX
	ADDQ         AX, DX
	ADDQ         AX, BX
	VMOVDQU      (CX), X0
	VMOVDQU      (DX), X1
	VMOVDQU      (BX), X2
	MOVQ         $0x8000000000000000, AX
	PINSRQ       $0x00, AX, X4
	VPBROADCASTQ X4, X4
	VPCMPEQQ     X1, X0, X3
	VPADDQ       X1, X4, X5
	VPADDQ       X0, X4, X6
	VPCMPGTQ     X5, X6, X5
	VMOVMSKPD    X3, AX
	VMOVMSKPD    X5, R9
	NOTL         AX
	BSFL         AX, SI
	TESTL        AX, AX
	BTSL         SI, R9
	JAE          part2
	VMOVDQU      X1, (CX)
	VMOVDQU      X0, (DX)
	VMOVDQA      X1, X3
	VMOVDQA      X0, X1
	VMOVDQA      X3, X0

part2:
	VPCMPEQQ  X2, X1, X3
	VPADDQ    X2, X4, X5
	VPADDQ    X1, X4, X6
	VPCMPGTQ  X5, X6, X5
	VMOVMSKPD X3, AX
	VMOVMSKPD X5, SI
	NOTL      AX
	BSFL      AX, DI
	TESTL     AX, AX
	BTSL      DI, SI
	JAE       done
	VMOVDQU   X2, (DX)
	VMOVDQU   X1, (BX)
	VPCMPEQQ  X2, X0, X1
	VPADDQ    X2, X4, X3
	VPADDQ    X0, X4, X4
	VPCMPGTQ  X3, X4, X3
	VMOVMSKPD X1, AX
	VMOVMSKPD X3, BX
	NOTL      AX
	BSFL      AX, R8
	TESTL     AX, AX
	BTSL      R8, BX
	JAE       done
	VMOVDQU   X2, (CX)
	VMOVDQU   X0, (DX)

done:
	RET

// func insertionsort16(data *byte, lo int, hi int)
// Requires: AVX, AVX2, SSE4.1
TEXT ·insertionsort16(SB), NOSPLIT, $0-24
	MOVQ         data+0(FP), AX
	MOVQ         lo+8(FP), CX
	MOVQ         hi+16(FP), DX
	SHLQ         $0x04, CX
	SHLQ         $0x04, DX
	LEAQ         (AX)(CX*1), CX
	LEAQ         (AX)(DX*1), AX
	MOVQ         $0x8000000000000000, DX
	PINSRQ       $0x00, DX, X0
	VPBROADCASTQ X0, X0
	MOVQ         CX, DX

outer:
	ADDQ    $0x10, DX
	CMPQ    DX, AX
	JA      done
	VMOVDQU (DX), X1
	MOVQ    DX, SI

inner:
	VMOVDQU   -16(SI), X2
	VPCMPEQQ  X1, X2, X3
	VPADDQ    X1, X0, X4
	VPADDQ    X2, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, DI
	VMOVMSKPD X4, R8
	NOTL      DI
	BSFL      DI, BX
	TESTL     DI, DI
	BTSL      BX, R8
	JAE       outer
	VMOVDQU   X2, (SI)
	VMOVDQU   X1, -16(SI)
	SUBQ      $0x10, SI
	CMPQ      SI, CX
	JA        inner
	JMP       outer

done:
	RET

// func distributeForward16(data *byte, scratch *byte, limit int, lo int, hi int, pivot int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeForward16(SB), NOSPLIT, $0-56
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	MOVQ         pivot+40(FP), DI
	SHLQ         $0x04, DX
	SHLQ         $0x04, BX
	SHLQ         $0x04, SI
	SHLQ         $0x04, DI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	LEAQ         -16(CX)(DX*1), CX
	MOVQ         $0x8000000000000000, R9
	PINSRQ       $0x00, R9, X0
	VPBROADCASTQ X0, X0
	VMOVDQU      (AX)(DI*1), X1
	XORQ         DI, DI
	XORQ         R9, R9
	NEGQ         DX

loop:
	VMOVDQU   (BX), X2
	VPCMPEQQ  X2, X1, X3
	VPADDQ    X2, X0, X4
	VPADDQ    X1, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, R10
	VMOVMSKPD X4, R11
	NOTL      R10
	BSFL      R10, R8
	TESTL     R10, R10
	BTSL      R8, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	XORB      $0x01, R9
	MOVQ      BX, R10
	CMOVQNE   CX, R10
	VMOVDQU   X2, (R10)(DI*1)
	SHLQ      $0x04, R9
	SUBQ      R9, DI
	ADDQ      $0x10, BX
	CMPQ      BX, SI
	JA        done
	CMPQ      DI, DX
	JNE       loop

done:
	SUBQ AX, BX
	ADDQ DI, BX
	SHRQ $0x04, BX
	DECQ BX
	MOVQ BX, ret+48(FP)
	RET

// func distributeBackward16(data *byte, scratch *byte, limit int, lo int, hi int, pivot int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeBackward16(SB), NOSPLIT, $0-56
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	MOVQ         pivot+40(FP), DI
	SHLQ         $0x04, DX
	SHLQ         $0x04, BX
	SHLQ         $0x04, SI
	SHLQ         $0x04, DI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	MOVQ         $0x8000000000000000, R9
	PINSRQ       $0x00, R9, X0
	VPBROADCASTQ X0, X0
	VMOVDQU      (AX)(DI*1), X1
	XORQ         DI, DI
	XORQ         R9, R9
	CMPQ         SI, BX
	JBE          done

loop:
	VMOVDQU   (SI), X2
	VPCMPEQQ  X2, X1, X3
	VPADDQ    X2, X0, X4
	VPADDQ    X1, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, R10
	VMOVMSKPD X4, R11
	NOTL      R10
	BSFL      R10, R8
	TESTL     R10, R10
	BTSL      R8, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	MOVQ      CX, R10
	CMOVQEQ   SI, R10
	VMOVDQU   X2, (R10)(DI*1)
	SHLQ      $0x04, R9
	ADDQ      R9, DI
	SUBQ      $0x10, SI
	CMPQ      SI, BX
	JBE       done
	CMPQ      DI, DX
	JNE       loop

done:
	SUBQ AX, SI
	ADDQ DI, SI
	SHRQ $0x04, SI
	MOVQ SI, ret+48(FP)
	RET

// func medianOfThree32(data *byte, a int, b int, c int)
// Requires: AVX, AVX2, SSE4.1
TEXT ·medianOfThree32(SB), NOSPLIT, $0-32
	MOVQ         data+0(FP), AX
	MOVQ         a+8(FP), CX
	MOVQ         b+16(FP), DX
	MOVQ         c+24(FP), BX
	SHLQ         $0x05, CX
	SHLQ         $0x05, DX
	SHLQ         $0x05, BX
	ADDQ         AX, CX
	ADDQ         AX, DX
	ADDQ         AX, BX
	VMOVDQU      (CX), Y0
	VMOVDQU      (DX), Y1
	VMOVDQU      (BX), Y2
	MOVQ         $0x8000000000000000, AX
	PINSRQ       $0x00, AX, X4
	VPBROADCASTQ X4, Y4
	VPCMPEQQ     Y1, Y0, Y3
	VPADDQ       Y1, Y4, Y5
	VPADDQ       Y0, Y4, Y6
	VPCMPGTQ     Y5, Y6, Y5
	VMOVMSKPD    Y3, AX
	VMOVMSKPD    Y5, R9
	NOTL         AX
	BSFL         AX, SI
	TESTL        AX, AX
	BTSL         SI, R9
	JAE          part2
	VMOVDQU      Y1, (CX)
	VMOVDQU      Y0, (DX)
	VMOVDQA      Y1, Y3
	VMOVDQA      Y0, Y1
	VMOVDQA      Y3, Y0

part2:
	VPCMPEQQ  Y2, Y1, Y3
	VPADDQ    Y2, Y4, Y5
	VPADDQ    Y1, Y4, Y6
	VPCMPGTQ  Y5, Y6, Y5
	VMOVMSKPD Y3, AX
	VMOVMSKPD Y5, SI
	NOTL      AX
	BSFL      AX, DI
	TESTL     AX, AX
	BTSL      DI, SI
	JAE       done
	VMOVDQU   Y2, (DX)
	VMOVDQU   Y1, (BX)
	VPCMPEQQ  Y2, Y0, Y1
	VPADDQ    Y2, Y4, Y3
	VPADDQ    Y0, Y4, Y4
	VPCMPGTQ  Y3, Y4, Y3
	VMOVMSKPD Y1, AX
	VMOVMSKPD Y3, BX
	NOTL      AX
	BSFL      AX, R8
	TESTL     AX, AX
	BTSL      R8, BX
	JAE       done
	VMOVDQU   Y2, (CX)
	VMOVDQU   Y0, (DX)

done:
	VZEROUPPER
	RET

// func insertionsort32(data *byte, lo int, hi int)
// Requires: AVX, AVX2, SSE4.1
TEXT ·insertionsort32(SB), NOSPLIT, $0-24
	MOVQ         data+0(FP), AX
	MOVQ         lo+8(FP), CX
	MOVQ         hi+16(FP), DX
	SHLQ         $0x05, CX
	SHLQ         $0x05, DX
	LEAQ         (AX)(CX*1), CX
	LEAQ         (AX)(DX*1), AX
	MOVQ         $0x8000000000000000, DX
	PINSRQ       $0x00, DX, X0
	VPBROADCASTQ X0, Y0
	MOVQ         CX, DX

outer:
	ADDQ    $0x20, DX
	CMPQ    DX, AX
	JA      done
	VMOVDQU (DX), Y1
	MOVQ    DX, SI

inner:
	VMOVDQU   -32(SI), Y2
	VPCMPEQQ  Y1, Y2, Y3
	VPADDQ    Y1, Y0, Y4
	VPADDQ    Y2, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, DI
	VMOVMSKPD Y4, R8
	NOTL      DI
	BSFL      DI, BX
	TESTL     DI, DI
	BTSL      BX, R8
	JAE       outer
	VMOVDQU   Y2, (SI)
	VMOVDQU   Y1, -32(SI)
	SUBQ      $0x20, SI
	CMPQ      SI, CX
	JA        inner
	JMP       outer

done:
	VZEROUPPER
	RET

// func distributeForward32(data *byte, scratch *byte, limit int, lo int, hi int, pivot int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeForward32(SB), NOSPLIT, $0-56
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	MOVQ         pivot+40(FP), DI
	SHLQ         $0x05, DX
	SHLQ         $0x05, BX
	SHLQ         $0x05, SI
	SHLQ         $0x05, DI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	LEAQ         -32(CX)(DX*1), CX
	MOVQ         $0x8000000000000000, R9
	PINSRQ       $0x00, R9, X0
	VPBROADCASTQ X0, Y0
	VMOVDQU      (AX)(DI*1), Y1
	XORQ         DI, DI
	XORQ         R9, R9
	NEGQ         DX

loop:
	VMOVDQU   (BX), Y2
	VPCMPEQQ  Y2, Y1, Y3
	VPADDQ    Y2, Y0, Y4
	VPADDQ    Y1, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, R10
	VMOVMSKPD Y4, R11
	NOTL      R10
	BSFL      R10, R8
	TESTL     R10, R10
	BTSL      R8, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	XORB      $0x01, R9
	MOVQ      BX, R10
	CMOVQNE   CX, R10
	VMOVDQU   Y2, (R10)(DI*1)
	SHLQ      $0x05, R9
	SUBQ      R9, DI
	ADDQ      $0x20, BX
	CMPQ      BX, SI
	JA        done
	CMPQ      DI, DX
	JNE       loop

done:
	SUBQ AX, BX
	ADDQ DI, BX
	SHRQ $0x05, BX
	DECQ BX
	MOVQ BX, ret+48(FP)
	VZEROUPPER
	RET

// func distributeBackward32(data *byte, scratch *byte, limit int, lo int, hi int, pivot int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeBackward32(SB), NOSPLIT, $0-56
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	MOVQ         pivot+40(FP), DI
	SHLQ         $0x05, DX
	SHLQ         $0x05, BX
	SHLQ         $0x05, SI
	SHLQ         $0x05, DI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	MOVQ         $0x8000000000000000, R9
	PINSRQ       $0x00, R9, X0
	VPBROADCASTQ X0, Y0
	VMOVDQU      (AX)(DI*1), Y1
	XORQ         DI, DI
	XORQ         R9, R9
	CMPQ         SI, BX
	JBE          done

loop:
	VMOVDQU   (SI), Y2
	VPCMPEQQ  Y2, Y1, Y3
	VPADDQ    Y2, Y0, Y4
	VPADDQ    Y1, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, R10
	VMOVMSKPD Y4, R11
	NOTL      R10
	BSFL      R10, R8
	TESTL     R10, R10
	BTSL      R8, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	MOVQ      CX, R10
	CMOVQEQ   SI, R10
	VMOVDQU   Y2, (R10)(DI*1)
	SHLQ      $0x05, R9
	ADDQ      R9, DI
	SUBQ      $0x20, SI
	CMPQ      SI, BX
	JBE       done
	CMPQ      DI, DX
	JNE       loop

done:
	SUBQ AX, SI
	ADDQ DI, SI
	SHRQ $0x05, SI
	MOVQ SI, ret+48(FP)
	VZEROUPPER
	RET
