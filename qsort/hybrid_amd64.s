// Code generated by command: go run hybrid_asm.go -pkg qsort -out ../qsort/hybrid_amd64.s -stubs ../qsort/hybrid_amd64.go. DO NOT EDIT.

#include "textflag.h"

// func insertionsort16(data *byte, lo int, hi int)
// Requires: AVX, AVX2, SSE4.1
TEXT ·insertionsort16(SB), NOSPLIT, $0-24
	MOVQ         data+0(FP), AX
	MOVQ         lo+8(FP), CX
	MOVQ         hi+16(FP), DX
	SHLQ         $0x04, CX
	SHLQ         $0x04, DX
	LEAQ         (AX)(CX*1), CX
	LEAQ         (AX)(DX*1), AX
	MOVQ         $0x8000000000000000, DX
	PINSRQ       $0x00, DX, X0
	VPBROADCASTQ X0, X0
	MOVQ         CX, DX

outer:
	ADDQ    $0x10, DX
	CMPQ    DX, AX
	JA      done
	VMOVDQU (DX), X1
	MOVQ    DX, SI

inner:
	VMOVDQU   -16(SI), X2
	VPCMPEQQ  X1, X2, X3
	VPADDQ    X1, X0, X4
	VPADDQ    X2, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, DI
	VMOVMSKPD X4, R8
	NOTL      DI
	BSFL      DI, BX
	TESTL     DI, DI
	BTSL      BX, R8
	JAE       outer
	VMOVDQU   X2, (SI)
	VMOVDQU   X1, -16(SI)
	SUBQ      $0x10, SI
	CMPQ      SI, CX
	JA        inner
	JMP       outer

done:
	RET

// func distributeForward16(data *byte, scratch *byte, limit int, lo int, hi int, pivot int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeForward16(SB), NOSPLIT, $0-56
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	MOVQ         pivot+40(FP), DI
	SHLQ         $0x04, DX
	SHLQ         $0x04, BX
	SHLQ         $0x04, SI
	SHLQ         $0x04, DI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	LEAQ         -16(CX)(DX*1), CX
	MOVQ         $0x8000000000000000, R9
	PINSRQ       $0x00, R9, X0
	VPBROADCASTQ X0, X0
	VMOVDQU      (AX)(DI*1), X1
	XORQ         DI, DI
	XORQ         R9, R9
	NEGQ         DX

loop:
	VMOVDQU   (BX), X2
	VPCMPEQQ  X2, X1, X3
	VPADDQ    X2, X0, X4
	VPADDQ    X1, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, R10
	VMOVMSKPD X4, R11
	NOTL      R10
	BSFL      R10, R8
	TESTL     R10, R10
	BTSL      R8, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	XORB      $0x01, R9
	MOVQ      BX, R10
	CMOVQNE   CX, R10
	VMOVDQU   X2, (R10)(DI*1)
	SHLQ      $0x04, R9
	SUBQ      R9, DI
	ADDQ      $0x10, BX
	CMPQ      BX, SI
	JA        done
	CMPQ      DI, DX
	JNE       loop

done:
	SUBQ AX, BX
	ADDQ DI, BX
	SHRQ $0x04, BX
	DECQ BX
	MOVQ BX, ret+48(FP)
	RET

// func distributeBackward16(data *byte, scratch *byte, limit int, lo int, hi int, pivot int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeBackward16(SB), NOSPLIT, $0-56
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	MOVQ         pivot+40(FP), DI
	SHLQ         $0x04, DX
	SHLQ         $0x04, BX
	SHLQ         $0x04, SI
	SHLQ         $0x04, DI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	MOVQ         $0x8000000000000000, R9
	PINSRQ       $0x00, R9, X0
	VPBROADCASTQ X0, X0
	VMOVDQU      (AX)(DI*1), X1
	XORQ         DI, DI
	XORQ         R9, R9
	CMPQ         SI, BX
	JBE          done

loop:
	VMOVDQU   (SI), X2
	VPCMPEQQ  X2, X1, X3
	VPADDQ    X2, X0, X4
	VPADDQ    X1, X0, X5
	VPCMPGTQ  X4, X5, X4
	VMOVMSKPD X3, R10
	VMOVMSKPD X4, R11
	NOTL      R10
	BSFL      R10, R8
	TESTL     R10, R10
	BTSL      R8, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	MOVQ      CX, R10
	CMOVQEQ   SI, R10
	VMOVDQU   X2, (R10)(DI*1)
	SHLQ      $0x04, R9
	ADDQ      R9, DI
	SUBQ      $0x10, SI
	CMPQ      SI, BX
	JBE       done
	CMPQ      DI, DX
	JNE       loop

done:
	SUBQ AX, SI
	ADDQ DI, SI
	SHRQ $0x04, SI
	MOVQ SI, ret+48(FP)
	RET

// func insertionsort32(data *byte, lo int, hi int)
// Requires: AVX, AVX2, SSE4.1
TEXT ·insertionsort32(SB), NOSPLIT, $0-24
	MOVQ         data+0(FP), AX
	MOVQ         lo+8(FP), CX
	MOVQ         hi+16(FP), DX
	SHLQ         $0x05, CX
	SHLQ         $0x05, DX
	LEAQ         (AX)(CX*1), CX
	LEAQ         (AX)(DX*1), AX
	MOVQ         $0x8000000000000000, DX
	PINSRQ       $0x00, DX, X0
	VPBROADCASTQ X0, Y0
	MOVQ         CX, DX

outer:
	ADDQ    $0x20, DX
	CMPQ    DX, AX
	JA      done
	VMOVDQU (DX), Y1
	MOVQ    DX, SI

inner:
	VMOVDQU   -32(SI), Y2
	VPCMPEQQ  Y1, Y2, Y3
	VPADDQ    Y1, Y0, Y4
	VPADDQ    Y2, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, DI
	VMOVMSKPD Y4, R8
	NOTL      DI
	BSFL      DI, BX
	TESTL     DI, DI
	BTSL      BX, R8
	JAE       outer
	VMOVDQU   Y2, (SI)
	VMOVDQU   Y1, -32(SI)
	SUBQ      $0x20, SI
	CMPQ      SI, CX
	JA        inner
	JMP       outer

done:
	VZEROUPPER
	RET

// func distributeForward32(data *byte, scratch *byte, limit int, lo int, hi int, pivot int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeForward32(SB), NOSPLIT, $0-56
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	MOVQ         pivot+40(FP), DI
	SHLQ         $0x05, DX
	SHLQ         $0x05, BX
	SHLQ         $0x05, SI
	SHLQ         $0x05, DI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	LEAQ         -32(CX)(DX*1), CX
	MOVQ         $0x8000000000000000, R9
	PINSRQ       $0x00, R9, X0
	VPBROADCASTQ X0, Y0
	VMOVDQU      (AX)(DI*1), Y1
	XORQ         DI, DI
	XORQ         R9, R9
	NEGQ         DX

loop:
	VMOVDQU   (BX), Y2
	VPCMPEQQ  Y2, Y1, Y3
	VPADDQ    Y2, Y0, Y4
	VPADDQ    Y1, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, R10
	VMOVMSKPD Y4, R11
	NOTL      R10
	BSFL      R10, R8
	TESTL     R10, R10
	BTSL      R8, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	XORB      $0x01, R9
	MOVQ      BX, R10
	CMOVQNE   CX, R10
	VMOVDQU   Y2, (R10)(DI*1)
	SHLQ      $0x05, R9
	SUBQ      R9, DI
	ADDQ      $0x20, BX
	CMPQ      BX, SI
	JA        done
	CMPQ      DI, DX
	JNE       loop

done:
	SUBQ AX, BX
	ADDQ DI, BX
	SHRQ $0x05, BX
	DECQ BX
	MOVQ BX, ret+48(FP)
	VZEROUPPER
	RET

// func distributeBackward32(data *byte, scratch *byte, limit int, lo int, hi int, pivot int) int
// Requires: AVX, AVX2, CMOV, SSE4.1
TEXT ·distributeBackward32(SB), NOSPLIT, $0-56
	MOVQ         data+0(FP), AX
	MOVQ         scratch+8(FP), CX
	MOVQ         limit+16(FP), DX
	MOVQ         lo+24(FP), BX
	MOVQ         hi+32(FP), SI
	MOVQ         pivot+40(FP), DI
	SHLQ         $0x05, DX
	SHLQ         $0x05, BX
	SHLQ         $0x05, SI
	SHLQ         $0x05, DI
	LEAQ         (AX)(BX*1), BX
	LEAQ         (AX)(SI*1), SI
	MOVQ         $0x8000000000000000, R9
	PINSRQ       $0x00, R9, X0
	VPBROADCASTQ X0, Y0
	VMOVDQU      (AX)(DI*1), Y1
	XORQ         DI, DI
	XORQ         R9, R9
	CMPQ         SI, BX
	JBE          done

loop:
	VMOVDQU   (SI), Y2
	VPCMPEQQ  Y2, Y1, Y3
	VPADDQ    Y2, Y0, Y4
	VPADDQ    Y1, Y0, Y5
	VPCMPGTQ  Y4, Y5, Y4
	VMOVMSKPD Y3, R10
	VMOVMSKPD Y4, R11
	NOTL      R10
	BSFL      R10, R8
	TESTL     R10, R10
	BTSL      R8, R11
	SETNE     R10
	SETCS     R9
	ANDB      R10, R9
	MOVQ      CX, R10
	CMOVQEQ   SI, R10
	VMOVDQU   Y2, (R10)(DI*1)
	SHLQ      $0x05, R9
	ADDQ      R9, DI
	SUBQ      $0x20, SI
	CMPQ      SI, BX
	JBE       done
	CMPQ      DI, DX
	JNE       loop

done:
	SUBQ AX, SI
	ADDQ DI, SI
	SHRQ $0x05, SI
	MOVQ SI, ret+48(FP)
	VZEROUPPER
	RET
