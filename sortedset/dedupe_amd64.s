// Code generated by command: go run dedupe_asm.go -pkg sortedset -out ../sortedset/dedupe_amd64.s -stubs ../sortedset/dedupe_amd64.go. DO NOT EDIT.

#include "textflag.h"

// func dedupe1(dst []byte, src []byte) int
// Requires: CMOV
TEXT ·dedupe1(SB), NOSPLIT, $0-56
	MOVQ src_len+32(FP), AX
	CMPQ AX, $0x00
	JE   short
	MOVQ dst_base+0(FP), CX
	MOVQ src_base+24(FP), DX
	MOVQ DX, BX
	MOVQ CX, SI
	ADDQ $0x01, DX
	SUBQ $0x01, AX
	MOVB (BX), DI
	MOVB DI, (SI)
	ADDQ $0x01, SI
	CMPQ AX, $0x00
	JE   done

generic:
	MOVQ    SI, DI
	ADDQ    $0x01, DI
	MOVB    (BX), R8
	MOVB    (DX), R9
	MOVB    R9, (SI)
	CMPB    R8, R9
	CMOVQNE DI, SI
	ADDQ    $0x01, BX
	ADDQ    $0x01, DX
	SUBQ    $0x01, AX
	CMPQ    AX, $0x00
	JG      generic

done:
	SUBQ CX, SI
	MOVQ SI, ret+48(FP)
	RET

short:
	MOVQ AX, ret+48(FP)
	RET

// func dedupe2(dst []byte, src []byte) int
// Requires: CMOV
TEXT ·dedupe2(SB), NOSPLIT, $0-56
	MOVQ src_len+32(FP), AX
	CMPQ AX, $0x00
	JE   short
	MOVQ dst_base+0(FP), CX
	MOVQ src_base+24(FP), DX
	MOVQ DX, BX
	MOVQ CX, SI
	ADDQ $0x02, DX
	SUBQ $0x02, AX
	MOVW (BX), DI
	MOVW DI, (SI)
	ADDQ $0x02, SI
	CMPQ AX, $0x00
	JE   done

generic:
	MOVQ    SI, DI
	ADDQ    $0x02, DI
	MOVW    (BX), R8
	MOVW    (DX), R9
	MOVW    R9, (SI)
	CMPW    R8, R9
	CMOVQNE DI, SI
	ADDQ    $0x02, BX
	ADDQ    $0x02, DX
	SUBQ    $0x02, AX
	CMPQ    AX, $0x00
	JG      generic

done:
	SUBQ CX, SI
	MOVQ SI, ret+48(FP)
	RET

short:
	MOVQ AX, ret+48(FP)
	RET

// func dedupe4(dst []byte, src []byte) int
// Requires: AVX, CMOV
TEXT ·dedupe4(SB), NOSPLIT, $8-56
	MOVQ src_len+32(FP), AX
	CMPQ AX, $0x00
	JE   short
	MOVQ dst_base+0(FP), CX
	MOVQ src_base+24(FP), DX
	MOVQ DX, BX
	MOVQ CX, SI
	ADDQ $0x04, DX
	SUBQ $0x04, AX
	CMPQ AX, $0x10
	JL   init
	BTL  $0x08, github·com∕segmentio∕asm∕cpu·X86+0(SB)
	JCS  avx2

init:
	MOVL (BX), DI
	MOVL DI, (SI)
	ADDQ $0x04, SI

tail:
	CMPQ AX, $0x00
	JE   done

generic:
	MOVQ    SI, DI
	ADDQ    $0x04, DI
	MOVL    (BX), R8
	MOVL    (DX), R9
	MOVL    R9, (SI)
	CMPL    R8, R9
	CMOVQNE DI, SI
	ADDQ    $0x04, BX
	ADDQ    $0x04, DX
	SUBQ    $0x04, AX
	CMPQ    AX, $0x00
	JG      generic

done:
	SUBQ CX, SI
	MOVQ SI, ret+48(FP)
	RET

short:
	MOVQ AX, ret+48(FP)
	RET

avx2:
	MOVL (BX), DI
	MOVL DI, (SI)
	LEAQ dedupe4_shuffle_mask<>+0(SB), R15
	LEAQ dedupe4_offset_array<>+0(SB), BP
	ADDQ $0x04, SI
	CMPQ AX, $0x00000080
	JL   avx2_tail64

avx2_loop128:
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   32(BX), X4
	VMOVDQU   48(BX), X6
	VMOVDQU   64(BX), X8
	VMOVDQU   80(BX), X10
	VMOVDQU   96(BX), X12
	VMOVDQU   112(BX), X14
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VMOVDQU   32(DX), X5
	VMOVDQU   48(DX), X7
	VMOVDQU   64(DX), X9
	VMOVDQU   80(DX), X11
	VMOVDQU   96(DX), X13
	VMOVDQU   112(DX), X15
	VPCMPEQD  X1, X0, X0
	VMOVMSKPS X0, DI
	SHLQ      $0x02, DI
	VPSHUFB   (R15)(DI*4), X1, X1
	MOVL      (BP)(DI*1), DI
	VPCMPEQD  X3, X2, X2
	VMOVMSKPS X2, R8
	SHLQ      $0x02, R8
	VPSHUFB   (R15)(R8*4), X3, X3
	MOVL      (BP)(R8*1), R8
	VPCMPEQD  X5, X4, X4
	VMOVMSKPS X4, R9
	SHLQ      $0x02, R9
	VPSHUFB   (R15)(R9*4), X5, X5
	MOVL      (BP)(R9*1), R9
	VPCMPEQD  X7, X6, X6
	VMOVMSKPS X6, R10
	SHLQ      $0x02, R10
	VPSHUFB   (R15)(R10*4), X7, X7
	MOVL      (BP)(R10*1), R10
	VPCMPEQD  X9, X8, X8
	VMOVMSKPS X8, R11
	SHLQ      $0x02, R11
	VPSHUFB   (R15)(R11*4), X9, X9
	MOVL      (BP)(R11*1), R11
	VPCMPEQD  X11, X10, X10
	VMOVMSKPS X10, R12
	SHLQ      $0x02, R12
	VPSHUFB   (R15)(R12*4), X11, X11
	MOVL      (BP)(R12*1), R12
	VPCMPEQD  X13, X12, X12
	VMOVMSKPS X12, R13
	SHLQ      $0x02, R13
	VPSHUFB   (R15)(R13*4), X13, X13
	MOVL      (BP)(R13*1), R13
	VPCMPEQD  X15, X14, X14
	VMOVMSKPS X14, R14
	SHLQ      $0x02, R14
	VPSHUFB   (R15)(R14*4), X15, X15
	MOVL      (BP)(R14*1), R14
	ADDQ      DI, R8
	ADDQ      R8, R9
	ADDQ      R9, R10
	ADDQ      R10, R11
	ADDQ      R11, R12
	ADDQ      R12, R13
	ADDQ      R13, R14
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	VMOVDQU   X5, (SI)(R8*1)
	VMOVDQU   X7, (SI)(R9*1)
	VMOVDQU   X9, (SI)(R10*1)
	VMOVDQU   X11, (SI)(R11*1)
	VMOVDQU   X13, (SI)(R12*1)
	VMOVDQU   X15, (SI)(R13*1)
	ADDQ      R14, SI
	ADDQ      $0x00000080, BX
	ADDQ      $0x00000080, DX
	SUBQ      $0x00000080, AX
	CMPQ      AX, $0x00000080
	JGE       avx2_loop128

avx2_tail64:
	CMPQ      AX, $0x40
	JL        avx2_tail32
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   32(BX), X4
	VMOVDQU   48(BX), X6
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VMOVDQU   32(DX), X5
	VMOVDQU   48(DX), X7
	VPCMPEQD  X1, X0, X0
	VMOVMSKPS X0, DI
	SHLQ      $0x02, DI
	VPSHUFB   (R15)(DI*4), X1, X1
	MOVL      (BP)(DI*1), DI
	VPCMPEQD  X3, X2, X2
	VMOVMSKPS X2, R8
	SHLQ      $0x02, R8
	VPSHUFB   (R15)(R8*4), X3, X3
	MOVL      (BP)(R8*1), R8
	VPCMPEQD  X5, X4, X4
	VMOVMSKPS X4, R9
	SHLQ      $0x02, R9
	VPSHUFB   (R15)(R9*4), X5, X5
	MOVL      (BP)(R9*1), R9
	VPCMPEQD  X7, X6, X6
	VMOVMSKPS X6, R10
	SHLQ      $0x02, R10
	VPSHUFB   (R15)(R10*4), X7, X7
	MOVL      (BP)(R10*1), R10
	ADDQ      DI, R8
	ADDQ      R8, R9
	ADDQ      R9, R10
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	VMOVDQU   X5, (SI)(R8*1)
	VMOVDQU   X7, (SI)(R9*1)
	ADDQ      R10, SI
	ADDQ      $0x40, BX
	ADDQ      $0x40, DX
	SUBQ      $0x40, AX

avx2_tail32:
	CMPQ      AX, $0x20
	JL        avx2_tail16
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VPCMPEQD  X1, X0, X0
	VMOVMSKPS X0, DI
	SHLQ      $0x02, DI
	VPSHUFB   (R15)(DI*4), X1, X1
	MOVL      (BP)(DI*1), DI
	VPCMPEQD  X3, X2, X2
	VMOVMSKPS X2, R8
	SHLQ      $0x02, R8
	VPSHUFB   (R15)(R8*4), X3, X3
	MOVL      (BP)(R8*1), R8
	ADDQ      DI, R8
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	ADDQ      R8, SI
	ADDQ      $0x20, BX
	ADDQ      $0x20, DX
	SUBQ      $0x20, AX

avx2_tail16:
	CMPQ      AX, $0x10
	JL        avx2_tail
	VMOVDQU   (BX), X0
	VMOVDQU   (DX), X1
	VPCMPEQD  X1, X0, X0
	VMOVMSKPS X0, DI
	SHLQ      $0x02, DI
	VPSHUFB   (R15)(DI*4), X1, X1
	MOVL      (BP)(DI*1), DI
	VMOVDQU   X1, (SI)
	ADDQ      DI, SI
	ADDQ      $0x10, BX
	ADDQ      $0x10, DX
	SUBQ      $0x10, AX

avx2_tail:
	VZEROUPPER
	JMP tail

DATA dedupe4_shuffle_mask<>+0(SB)/8, $0x0706050403020100
DATA dedupe4_shuffle_mask<>+8(SB)/8, $0x0f0e0d0c0b0a0908
DATA dedupe4_shuffle_mask<>+16(SB)/8, $0x0b0a090807060504
DATA dedupe4_shuffle_mask<>+24(SB)/8, $0x030201000f0e0d0c
DATA dedupe4_shuffle_mask<>+32(SB)/8, $0x0b0a090803020100
DATA dedupe4_shuffle_mask<>+40(SB)/8, $0x070605040f0e0d0c
DATA dedupe4_shuffle_mask<>+48(SB)/8, $0x0f0e0d0c0b0a0908
DATA dedupe4_shuffle_mask<>+56(SB)/8, $0x0706050403020100
DATA dedupe4_shuffle_mask<>+64(SB)/8, $0x0706050403020100
DATA dedupe4_shuffle_mask<>+72(SB)/8, $0x0b0a09080f0e0d0c
DATA dedupe4_shuffle_mask<>+80(SB)/8, $0x0f0e0d0c07060504
DATA dedupe4_shuffle_mask<>+88(SB)/8, $0x0b0a090803020100
DATA dedupe4_shuffle_mask<>+96(SB)/8, $0x0f0e0d0c03020100
DATA dedupe4_shuffle_mask<>+104(SB)/8, $0x0b0a090807060504
DATA dedupe4_shuffle_mask<>+112(SB)/8, $0x030201000f0e0d0c
DATA dedupe4_shuffle_mask<>+120(SB)/8, $0x0b0a090807060504
DATA dedupe4_shuffle_mask<>+128(SB)/8, $0x0706050403020100
DATA dedupe4_shuffle_mask<>+136(SB)/8, $0x0f0e0d0c0b0a0908
DATA dedupe4_shuffle_mask<>+144(SB)/8, $0x0b0a090807060504
DATA dedupe4_shuffle_mask<>+152(SB)/8, $0x0f0e0d0c03020100
DATA dedupe4_shuffle_mask<>+160(SB)/8, $0x0f0e0d0c03020100
DATA dedupe4_shuffle_mask<>+168(SB)/8, $0x0b0a090807060504
DATA dedupe4_shuffle_mask<>+176(SB)/8, $0x030201000b0a0908
DATA dedupe4_shuffle_mask<>+184(SB)/8, $0x0f0e0d0c07060504
DATA dedupe4_shuffle_mask<>+192(SB)/8, $0x0706050403020100
DATA dedupe4_shuffle_mask<>+200(SB)/8, $0x0f0e0d0c0b0a0908
DATA dedupe4_shuffle_mask<>+208(SB)/8, $0x0302010007060504
DATA dedupe4_shuffle_mask<>+216(SB)/8, $0x0f0e0d0c0b0a0908
DATA dedupe4_shuffle_mask<>+224(SB)/8, $0x0706050403020100
DATA dedupe4_shuffle_mask<>+232(SB)/8, $0x0f0e0d0c0b0a0908
DATA dedupe4_shuffle_mask<>+240(SB)/8, $0x0706050403020100
DATA dedupe4_shuffle_mask<>+248(SB)/8, $0x0f0e0d0c0b0a0908
GLOBL dedupe4_shuffle_mask<>(SB), RODATA|NOPTR, $256

DATA dedupe4_offset_array<>+0(SB)/8, $0x0000000c00000010
DATA dedupe4_offset_array<>+8(SB)/8, $0x000000080000000c
DATA dedupe4_offset_array<>+16(SB)/8, $0x000000080000000c
DATA dedupe4_offset_array<>+24(SB)/8, $0x0000000400000008
DATA dedupe4_offset_array<>+32(SB)/8, $0x000000080000000c
DATA dedupe4_offset_array<>+40(SB)/8, $0x0000000400000008
DATA dedupe4_offset_array<>+48(SB)/8, $0x0000000400000008
DATA dedupe4_offset_array<>+56(SB)/8, $0x0000000000000004
GLOBL dedupe4_offset_array<>(SB), RODATA|NOPTR, $64

// func dedupe8(dst []byte, src []byte) int
// Requires: AVX, CMOV
TEXT ·dedupe8(SB), NOSPLIT, $8-56
	MOVQ src_len+32(FP), AX
	CMPQ AX, $0x00
	JE   short
	MOVQ dst_base+0(FP), CX
	MOVQ src_base+24(FP), DX
	MOVQ DX, BX
	MOVQ CX, SI
	ADDQ $0x08, DX
	SUBQ $0x08, AX
	CMPQ AX, $0x10
	JL   init
	BTL  $0x08, github·com∕segmentio∕asm∕cpu·X86+0(SB)
	JCS  avx2

init:
	MOVQ (BX), DI
	MOVQ DI, (SI)
	ADDQ $0x08, SI

tail:
	CMPQ AX, $0x00
	JE   done

generic:
	MOVQ    SI, DI
	ADDQ    $0x08, DI
	MOVQ    (BX), R8
	MOVQ    (DX), R9
	MOVQ    R9, (SI)
	CMPQ    R8, R9
	CMOVQNE DI, SI
	ADDQ    $0x08, BX
	ADDQ    $0x08, DX
	SUBQ    $0x08, AX
	CMPQ    AX, $0x00
	JG      generic

done:
	SUBQ CX, SI
	MOVQ SI, ret+48(FP)
	RET

short:
	MOVQ AX, ret+48(FP)
	RET

avx2:
	MOVQ (BX), DI
	MOVQ DI, (SI)
	LEAQ dedupe8_shuffle_mask<>+0(SB), R15
	LEAQ dedupe8_offset_array<>+0(SB), BP
	ADDQ $0x08, SI
	CMPQ AX, $0x00000080
	JL   avx2_tail64

avx2_loop128:
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   32(BX), X4
	VMOVDQU   48(BX), X6
	VMOVDQU   64(BX), X8
	VMOVDQU   80(BX), X10
	VMOVDQU   96(BX), X12
	VMOVDQU   112(BX), X14
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VMOVDQU   32(DX), X5
	VMOVDQU   48(DX), X7
	VMOVDQU   64(DX), X9
	VMOVDQU   80(DX), X11
	VMOVDQU   96(DX), X13
	VMOVDQU   112(DX), X15
	VPCMPEQQ  X1, X0, X0
	VMOVMSKPD X0, DI
	VPSHUFB   (R15)(DI*8), X1, X1
	MOVQ      (BP)(DI*8), DI
	VPCMPEQQ  X3, X2, X2
	VMOVMSKPD X2, R8
	VPSHUFB   (R15)(R8*8), X3, X3
	MOVQ      (BP)(R8*8), R8
	VPCMPEQQ  X5, X4, X4
	VMOVMSKPD X4, R9
	VPSHUFB   (R15)(R9*8), X5, X5
	MOVQ      (BP)(R9*8), R9
	VPCMPEQQ  X7, X6, X6
	VMOVMSKPD X6, R10
	VPSHUFB   (R15)(R10*8), X7, X7
	MOVQ      (BP)(R10*8), R10
	VPCMPEQQ  X9, X8, X8
	VMOVMSKPD X8, R11
	VPSHUFB   (R15)(R11*8), X9, X9
	MOVQ      (BP)(R11*8), R11
	VPCMPEQQ  X11, X10, X10
	VMOVMSKPD X10, R12
	VPSHUFB   (R15)(R12*8), X11, X11
	MOVQ      (BP)(R12*8), R12
	VPCMPEQQ  X13, X12, X12
	VMOVMSKPD X12, R13
	VPSHUFB   (R15)(R13*8), X13, X13
	MOVQ      (BP)(R13*8), R13
	VPCMPEQQ  X15, X14, X14
	VMOVMSKPD X14, R14
	VPSHUFB   (R15)(R14*8), X15, X15
	MOVQ      (BP)(R14*8), R14
	ADDQ      DI, R8
	ADDQ      R8, R9
	ADDQ      R9, R10
	ADDQ      R10, R11
	ADDQ      R11, R12
	ADDQ      R12, R13
	ADDQ      R13, R14
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	VMOVDQU   X5, (SI)(R8*1)
	VMOVDQU   X7, (SI)(R9*1)
	VMOVDQU   X9, (SI)(R10*1)
	VMOVDQU   X11, (SI)(R11*1)
	VMOVDQU   X13, (SI)(R12*1)
	VMOVDQU   X15, (SI)(R13*1)
	ADDQ      R14, SI
	ADDQ      $0x00000080, BX
	ADDQ      $0x00000080, DX
	SUBQ      $0x00000080, AX
	CMPQ      AX, $0x00000080
	JGE       avx2_loop128

avx2_tail64:
	CMPQ      AX, $0x40
	JL        avx2_tail32
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   32(BX), X4
	VMOVDQU   48(BX), X6
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VMOVDQU   32(DX), X5
	VMOVDQU   48(DX), X7
	VPCMPEQQ  X1, X0, X0
	VMOVMSKPD X0, DI
	VPSHUFB   (R15)(DI*8), X1, X1
	MOVQ      (BP)(DI*8), DI
	VPCMPEQQ  X3, X2, X2
	VMOVMSKPD X2, R8
	VPSHUFB   (R15)(R8*8), X3, X3
	MOVQ      (BP)(R8*8), R8
	VPCMPEQQ  X5, X4, X4
	VMOVMSKPD X4, R9
	VPSHUFB   (R15)(R9*8), X5, X5
	MOVQ      (BP)(R9*8), R9
	VPCMPEQQ  X7, X6, X6
	VMOVMSKPD X6, R10
	VPSHUFB   (R15)(R10*8), X7, X7
	MOVQ      (BP)(R10*8), R10
	ADDQ      DI, R8
	ADDQ      R8, R9
	ADDQ      R9, R10
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	VMOVDQU   X5, (SI)(R8*1)
	VMOVDQU   X7, (SI)(R9*1)
	ADDQ      R10, SI
	ADDQ      $0x40, BX
	ADDQ      $0x40, DX
	SUBQ      $0x40, AX

avx2_tail32:
	CMPQ      AX, $0x20
	JL        avx2_tail16
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VPCMPEQQ  X1, X0, X0
	VMOVMSKPD X0, DI
	VPSHUFB   (R15)(DI*8), X1, X1
	MOVQ      (BP)(DI*8), DI
	VPCMPEQQ  X3, X2, X2
	VMOVMSKPD X2, R8
	VPSHUFB   (R15)(R8*8), X3, X3
	MOVQ      (BP)(R8*8), R8
	ADDQ      DI, R8
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	ADDQ      R8, SI
	ADDQ      $0x20, BX
	ADDQ      $0x20, DX
	SUBQ      $0x20, AX

avx2_tail16:
	CMPQ      AX, $0x10
	JL        avx2_tail
	VMOVDQU   (BX), X0
	VMOVDQU   (DX), X1
	VPCMPEQQ  X1, X0, X0
	VMOVMSKPD X0, DI
	VPSHUFB   (R15)(DI*8), X1, X1
	MOVQ      (BP)(DI*8), DI
	VMOVDQU   X1, (SI)
	ADDQ      DI, SI
	ADDQ      $0x10, BX
	ADDQ      $0x10, DX
	SUBQ      $0x10, AX

avx2_tail:
	VZEROUPPER
	JMP tail

DATA dedupe8_shuffle_mask<>+0(SB)/8, $0x0706050403020100
DATA dedupe8_shuffle_mask<>+8(SB)/8, $0x0f0e0d0c0b0a0908
DATA dedupe8_shuffle_mask<>+16(SB)/8, $0x0706050403020100
DATA dedupe8_shuffle_mask<>+24(SB)/8, $0x0706050403020100
DATA dedupe8_shuffle_mask<>+32(SB)/8, $0x0706050403020100
GLOBL dedupe8_shuffle_mask<>(SB), RODATA|NOPTR, $40

DATA dedupe8_offset_array<>+0(SB)/8, $0x0000000000000010
DATA dedupe8_offset_array<>+8(SB)/8, $0x0000000000000008
DATA dedupe8_offset_array<>+16(SB)/8, $0x0000000000000008
DATA dedupe8_offset_array<>+24(SB)/8, $0x0000000000000000
GLOBL dedupe8_offset_array<>(SB), RODATA|NOPTR, $32

// func dedupe16(dst []byte, src []byte) int
// Requires: AVX, CMOV, SSE2, SSE4.1
TEXT ·dedupe16(SB), NOSPLIT, $8-56
	MOVQ src_len+32(FP), AX
	CMPQ AX, $0x00
	JE   short
	MOVQ dst_base+0(FP), CX
	MOVQ src_base+24(FP), DX
	MOVQ DX, BX
	MOVQ CX, SI
	ADDQ $0x10, DX
	SUBQ $0x10, AX
	CMPQ AX, $0x10
	JL   init
	BTL  $0x08, github·com∕segmentio∕asm∕cpu·X86+0(SB)
	JCS  avx2

init:
	MOVOU (BX), X0
	MOVOU X0, (SI)
	ADDQ  $0x10, SI

tail:
	CMPQ AX, $0x00
	JE   done

generic:
	MOVQ     SI, DI
	ADDQ     $0x10, DI
	MOVOU    (BX), X0
	MOVOU    (DX), X1
	MOVOU    X1, (SI)
	PCMPEQQ  X0, X1
	PMOVMSKB X1, R8
	CMPL     R8, $0x0000ffff
	CMOVQNE  DI, SI
	ADDQ     $0x10, BX
	ADDQ     $0x10, DX
	SUBQ     $0x10, AX
	CMPQ     AX, $0x00
	JG       generic

done:
	SUBQ CX, SI
	MOVQ SI, ret+48(FP)
	RET

short:
	MOVQ AX, ret+48(FP)
	RET

avx2:
	VMOVDQU (BX), X0
	VMOVDQU X0, (SI)
	XORQ    R15, R15
	MOVQ    $0x0000000000000010, BP
	ADDQ    $0x10, SI
	CMPQ    AX, $0x00000080
	JL      avx2_tail64

avx2_loop128:
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   32(BX), X4
	VMOVDQU   48(BX), X6
	VMOVDQU   64(BX), X8
	VMOVDQU   80(BX), X10
	VMOVDQU   96(BX), X12
	VMOVDQU   112(BX), X14
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VMOVDQU   32(DX), X5
	VMOVDQU   48(DX), X7
	VMOVDQU   64(DX), X9
	VMOVDQU   80(DX), X11
	VMOVDQU   96(DX), X13
	VMOVDQU   112(DX), X15
	VPCMPEQQ  X1, X0, X0
	VMOVMSKPD X0, DI
	CMPQ      DI, $0x03
	CMOVQEQ   R15, DI
	CMOVQNE   BP, DI
	VPCMPEQQ  X3, X2, X2
	VMOVMSKPD X2, R8
	CMPQ      R8, $0x03
	CMOVQEQ   R15, R8
	CMOVQNE   BP, R8
	VPCMPEQQ  X5, X4, X4
	VMOVMSKPD X4, R9
	CMPQ      R9, $0x03
	CMOVQEQ   R15, R9
	CMOVQNE   BP, R9
	VPCMPEQQ  X7, X6, X6
	VMOVMSKPD X6, R10
	CMPQ      R10, $0x03
	CMOVQEQ   R15, R10
	CMOVQNE   BP, R10
	VPCMPEQQ  X9, X8, X8
	VMOVMSKPD X8, R11
	CMPQ      R11, $0x03
	CMOVQEQ   R15, R11
	CMOVQNE   BP, R11
	VPCMPEQQ  X11, X10, X10
	VMOVMSKPD X10, R12
	CMPQ      R12, $0x03
	CMOVQEQ   R15, R12
	CMOVQNE   BP, R12
	VPCMPEQQ  X13, X12, X12
	VMOVMSKPD X12, R13
	CMPQ      R13, $0x03
	CMOVQEQ   R15, R13
	CMOVQNE   BP, R13
	VPCMPEQQ  X15, X14, X14
	VMOVMSKPD X14, R14
	CMPQ      R14, $0x03
	CMOVQEQ   R15, R14
	CMOVQNE   BP, R14
	ADDQ      DI, R8
	ADDQ      R8, R9
	ADDQ      R9, R10
	ADDQ      R10, R11
	ADDQ      R11, R12
	ADDQ      R12, R13
	ADDQ      R13, R14
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	VMOVDQU   X5, (SI)(R8*1)
	VMOVDQU   X7, (SI)(R9*1)
	VMOVDQU   X9, (SI)(R10*1)
	VMOVDQU   X11, (SI)(R11*1)
	VMOVDQU   X13, (SI)(R12*1)
	VMOVDQU   X15, (SI)(R13*1)
	ADDQ      R14, SI
	ADDQ      $0x00000080, BX
	ADDQ      $0x00000080, DX
	SUBQ      $0x00000080, AX
	CMPQ      AX, $0x00000080
	JGE       avx2_loop128

avx2_tail64:
	CMPQ      AX, $0x40
	JL        avx2_tail32
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   32(BX), X4
	VMOVDQU   48(BX), X6
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VMOVDQU   32(DX), X5
	VMOVDQU   48(DX), X7
	VPCMPEQQ  X1, X0, X0
	VMOVMSKPD X0, DI
	CMPQ      DI, $0x03
	CMOVQEQ   R15, DI
	CMOVQNE   BP, DI
	VPCMPEQQ  X3, X2, X2
	VMOVMSKPD X2, R8
	CMPQ      R8, $0x03
	CMOVQEQ   R15, R8
	CMOVQNE   BP, R8
	VPCMPEQQ  X5, X4, X4
	VMOVMSKPD X4, R9
	CMPQ      R9, $0x03
	CMOVQEQ   R15, R9
	CMOVQNE   BP, R9
	VPCMPEQQ  X7, X6, X6
	VMOVMSKPD X6, R10
	CMPQ      R10, $0x03
	CMOVQEQ   R15, R10
	CMOVQNE   BP, R10
	ADDQ      DI, R8
	ADDQ      R8, R9
	ADDQ      R9, R10
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	VMOVDQU   X5, (SI)(R8*1)
	VMOVDQU   X7, (SI)(R9*1)
	ADDQ      R10, SI
	ADDQ      $0x40, BX
	ADDQ      $0x40, DX
	SUBQ      $0x40, AX

avx2_tail32:
	CMPQ      AX, $0x20
	JL        avx2_tail16
	VMOVDQU   (BX), X0
	VMOVDQU   16(BX), X2
	VMOVDQU   (DX), X1
	VMOVDQU   16(DX), X3
	VPCMPEQQ  X1, X0, X0
	VMOVMSKPD X0, DI
	CMPQ      DI, $0x03
	CMOVQEQ   R15, DI
	CMOVQNE   BP, DI
	VPCMPEQQ  X3, X2, X2
	VMOVMSKPD X2, R8
	CMPQ      R8, $0x03
	CMOVQEQ   R15, R8
	CMOVQNE   BP, R8
	ADDQ      DI, R8
	VMOVDQU   X1, (SI)
	VMOVDQU   X3, (SI)(DI*1)
	ADDQ      R8, SI
	ADDQ      $0x20, BX
	ADDQ      $0x20, DX
	SUBQ      $0x20, AX

avx2_tail16:
	CMPQ      AX, $0x10
	JL        avx2_tail
	VMOVDQU   (BX), X0
	VMOVDQU   (DX), X1
	VPCMPEQQ  X1, X0, X0
	VMOVMSKPD X0, DI
	CMPQ      DI, $0x03
	CMOVQEQ   R15, DI
	CMOVQNE   BP, DI
	VMOVDQU   X1, (SI)
	ADDQ      DI, SI
	ADDQ      $0x10, BX
	ADDQ      $0x10, DX
	SUBQ      $0x10, AX

avx2_tail:
	VZEROUPPER
	JMP tail

// func dedupe32(dst []byte, src []byte) int
// Requires: AVX, AVX2, CMOV, SSE2, SSE4.1
TEXT ·dedupe32(SB), NOSPLIT, $8-56
	MOVQ src_len+32(FP), AX
	CMPQ AX, $0x00
	JE   short
	MOVQ dst_base+0(FP), CX
	MOVQ src_base+24(FP), DX
	MOVQ DX, BX
	MOVQ CX, SI
	ADDQ $0x20, DX
	SUBQ $0x20, AX
	CMPQ AX, $0x20
	JL   init
	BTL  $0x08, github·com∕segmentio∕asm∕cpu·X86+0(SB)
	JCS  avx2

init:
	MOVOU (BX), X0
	MOVOU 16(BX), X1
	MOVOU X0, (SI)
	MOVOU X1, 16(SI)
	ADDQ  $0x20, SI

tail:
	CMPQ AX, $0x00
	JE   done

generic:
	MOVQ     SI, DI
	ADDQ     $0x20, DI
	MOVOU    (DX), X2
	MOVOU    16(DX), X3
	MOVOU    (BX), X0
	MOVOU    16(BX), X1
	MOVOU    X2, (SI)
	MOVOU    X3, 16(SI)
	PCMPEQQ  X0, X2
	PCMPEQQ  X1, X3
	PMOVMSKB X2, R8
	PMOVMSKB X3, R9
	ANDL     R9, R8
	CMPL     R8, $0x0000ffff
	CMOVQNE  DI, SI
	ADDQ     $0x20, BX
	ADDQ     $0x20, DX
	SUBQ     $0x20, AX
	CMPQ     AX, $0x00
	JG       generic

done:
	SUBQ CX, SI
	MOVQ SI, ret+48(FP)
	RET

short:
	MOVQ AX, ret+48(FP)
	RET

avx2:
	VMOVDQU (BX), Y0
	VMOVDQU Y0, (SI)
	XORQ    R15, R15
	MOVQ    $0x0000000000000020, BP
	ADDQ    $0x20, SI
	CMPQ    AX, $0x00000100
	JL      avx2_tail128

avx2_loop256:
	VMOVDQU   (BX), Y0
	VMOVDQU   32(BX), Y2
	VMOVDQU   64(BX), Y4
	VMOVDQU   96(BX), Y6
	VMOVDQU   128(BX), Y8
	VMOVDQU   160(BX), Y10
	VMOVDQU   192(BX), Y12
	VMOVDQU   224(BX), Y14
	VMOVDQU   (DX), Y1
	VMOVDQU   32(DX), Y3
	VMOVDQU   64(DX), Y5
	VMOVDQU   96(DX), Y7
	VMOVDQU   128(DX), Y9
	VMOVDQU   160(DX), Y11
	VMOVDQU   192(DX), Y13
	VMOVDQU   224(DX), Y15
	VPCMPEQQ  Y1, Y0, Y0
	VMOVMSKPD Y0, DI
	CMPQ      DI, $0x0f
	CMOVQEQ   R15, DI
	CMOVQNE   BP, DI
	VPCMPEQQ  Y3, Y2, Y2
	VMOVMSKPD Y2, R8
	CMPQ      R8, $0x0f
	CMOVQEQ   R15, R8
	CMOVQNE   BP, R8
	VPCMPEQQ  Y5, Y4, Y4
	VMOVMSKPD Y4, R9
	CMPQ      R9, $0x0f
	CMOVQEQ   R15, R9
	CMOVQNE   BP, R9
	VPCMPEQQ  Y7, Y6, Y6
	VMOVMSKPD Y6, R10
	CMPQ      R10, $0x0f
	CMOVQEQ   R15, R10
	CMOVQNE   BP, R10
	VPCMPEQQ  Y9, Y8, Y8
	VMOVMSKPD Y8, R11
	CMPQ      R11, $0x0f
	CMOVQEQ   R15, R11
	CMOVQNE   BP, R11
	VPCMPEQQ  Y11, Y10, Y10
	VMOVMSKPD Y10, R12
	CMPQ      R12, $0x0f
	CMOVQEQ   R15, R12
	CMOVQNE   BP, R12
	VPCMPEQQ  Y13, Y12, Y12
	VMOVMSKPD Y12, R13
	CMPQ      R13, $0x0f
	CMOVQEQ   R15, R13
	CMOVQNE   BP, R13
	VPCMPEQQ  Y15, Y14, Y14
	VMOVMSKPD Y14, R14
	CMPQ      R14, $0x0f
	CMOVQEQ   R15, R14
	CMOVQNE   BP, R14
	ADDQ      DI, R8
	ADDQ      R8, R9
	ADDQ      R9, R10
	ADDQ      R10, R11
	ADDQ      R11, R12
	ADDQ      R12, R13
	ADDQ      R13, R14
	VMOVDQU   Y1, (SI)
	VMOVDQU   Y3, (SI)(DI*1)
	VMOVDQU   Y5, (SI)(R8*1)
	VMOVDQU   Y7, (SI)(R9*1)
	VMOVDQU   Y9, (SI)(R10*1)
	VMOVDQU   Y11, (SI)(R11*1)
	VMOVDQU   Y13, (SI)(R12*1)
	VMOVDQU   Y15, (SI)(R13*1)
	ADDQ      R14, SI
	ADDQ      $0x00000100, BX
	ADDQ      $0x00000100, DX
	SUBQ      $0x00000100, AX
	CMPQ      AX, $0x00000100
	JGE       avx2_loop256

avx2_tail128:
	CMPQ      AX, $0x80
	JL        avx2_tail64
	VMOVDQU   (BX), Y0
	VMOVDQU   32(BX), Y2
	VMOVDQU   64(BX), Y4
	VMOVDQU   96(BX), Y6
	VMOVDQU   (DX), Y1
	VMOVDQU   32(DX), Y3
	VMOVDQU   64(DX), Y5
	VMOVDQU   96(DX), Y7
	VPCMPEQQ  Y1, Y0, Y0
	VMOVMSKPD Y0, DI
	CMPQ      DI, $0x0f
	CMOVQEQ   R15, DI
	CMOVQNE   BP, DI
	VPCMPEQQ  Y3, Y2, Y2
	VMOVMSKPD Y2, R8
	CMPQ      R8, $0x0f
	CMOVQEQ   R15, R8
	CMOVQNE   BP, R8
	VPCMPEQQ  Y5, Y4, Y4
	VMOVMSKPD Y4, R9
	CMPQ      R9, $0x0f
	CMOVQEQ   R15, R9
	CMOVQNE   BP, R9
	VPCMPEQQ  Y7, Y6, Y6
	VMOVMSKPD Y6, R10
	CMPQ      R10, $0x0f
	CMOVQEQ   R15, R10
	CMOVQNE   BP, R10
	ADDQ      DI, R8
	ADDQ      R8, R9
	ADDQ      R9, R10
	VMOVDQU   Y1, (SI)
	VMOVDQU   Y3, (SI)(DI*1)
	VMOVDQU   Y5, (SI)(R8*1)
	VMOVDQU   Y7, (SI)(R9*1)
	ADDQ      R10, SI
	ADDQ      $0x80, BX
	ADDQ      $0x80, DX
	SUBQ      $0x80, AX

avx2_tail64:
	CMPQ      AX, $0x40
	JL        avx2_tail32
	VMOVDQU   (BX), Y0
	VMOVDQU   32(BX), Y2
	VMOVDQU   (DX), Y1
	VMOVDQU   32(DX), Y3
	VPCMPEQQ  Y1, Y0, Y0
	VMOVMSKPD Y0, DI
	CMPQ      DI, $0x0f
	CMOVQEQ   R15, DI
	CMOVQNE   BP, DI
	VPCMPEQQ  Y3, Y2, Y2
	VMOVMSKPD Y2, R8
	CMPQ      R8, $0x0f
	CMOVQEQ   R15, R8
	CMOVQNE   BP, R8
	ADDQ      DI, R8
	VMOVDQU   Y1, (SI)
	VMOVDQU   Y3, (SI)(DI*1)
	ADDQ      R8, SI
	ADDQ      $0x40, BX
	ADDQ      $0x40, DX
	SUBQ      $0x40, AX

avx2_tail32:
	CMPQ      AX, $0x20
	JL        avx2_tail
	VMOVDQU   (BX), Y0
	VMOVDQU   (DX), Y1
	VPCMPEQQ  Y1, Y0, Y0
	VMOVMSKPD Y0, DI
	CMPQ      DI, $0x0f
	CMOVQEQ   R15, DI
	CMOVQNE   BP, DI
	VMOVDQU   Y1, (SI)
	ADDQ      DI, SI
	ADDQ      $0x20, BX
	ADDQ      $0x20, DX
	SUBQ      $0x20, AX

avx2_tail:
	VZEROUPPER
	JMP tail
